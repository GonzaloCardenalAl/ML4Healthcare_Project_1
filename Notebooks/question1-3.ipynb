{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_preprocess_without_scaling(df, feature_columns, id_columns, categorical_columns, \n",
    "                                     categorical_modes, stats=None, random_state=42):\n",
    "    \"\"\"\n",
    "    Helper function to preprocess data without scaling using improved method\n",
    "    Now uses normal distribution sampling instead of backward filling\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Make sure categorical columns don't have missing values first\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if col in df_processed.columns and df_processed[col].isna().sum() > 0:\n",
    "            # Fill categorical missing values with the most frequent value from training\n",
    "            df_processed[col].fillna(categorical_modes[col], inplace=True)\n",
    "    \n",
    "    # Step 1: Sort data by patient and time to ensure correct filling\n",
    "    df_sorted = df_processed.sort_values(['PatientID', 'Hours'])\n",
    "    df_processed = df_sorted.copy()\n",
    "    \n",
    "    # Apply forward filling within each patient group\n",
    "    print(\"Applying forward filling...\")\n",
    "    for patient_id in df_processed['PatientID'].unique():\n",
    "        patient_mask = df_processed['PatientID'] == patient_id\n",
    "        df_processed.loc[patient_mask, feature_columns] = df_processed.loc[patient_mask, feature_columns].ffill()\n",
    "    \n",
    "    print(f\"Missing values after forward filling: {df_processed[feature_columns].isna().sum().sum()}\")\n",
    "    \n",
    "    # Instead of backward filling, sample from normal distribution for remaining NAs\n",
    "    print(\"Sampling from normal distribution for remaining NAs...\")\n",
    "    \n",
    "    # Get remaining nulls count before sampling\n",
    "    remaining_nulls_before = df_processed[feature_columns].isna().sum().sum()\n",
    "    \n",
    "    # Define non-negative features\n",
    "    non_negative_features = ['Glucose', 'BUN', 'Creatinine', 'AST', 'ALT', 'ALP', 'Bilirubin', \n",
    "                           'Troponin', 'Lactate', 'PaO2', 'Urine', 'WBC', 'Platelets', \n",
    "                           'O2Sat', 'GCS', 'Age']\n",
    "    \n",
    "    for col in feature_columns:\n",
    "        # Check if we have nulls for this column\n",
    "        null_mask = df_processed[col].isna()\n",
    "        num_nulls = null_mask.sum()\n",
    "        \n",
    "        if num_nulls > 0:\n",
    "            # If we have stats for normal distribution parameters\n",
    "            if stats is not None and col in stats:\n",
    "                mean_val = stats[col]['mean']\n",
    "                std_val = stats[col]['std']\n",
    "                \n",
    "                # Add small epsilon to std to avoid division by zero\n",
    "                epsilon = 1e-6\n",
    "                \n",
    "                # Generate samples from normal distribution\n",
    "                samples = np.random.normal(mean_val, std_val + epsilon, size=num_nulls)\n",
    "                \n",
    "                # Clip at zero for non-negative features\n",
    "                if col in non_negative_features:\n",
    "                    samples = np.clip(samples, 0, None)\n",
    "                \n",
    "                # Assign sampled values to nulls\n",
    "                df_processed.loc[null_mask, col] = samples\n",
    "                \n",
    "                print(f\"Sampled {num_nulls} values for {col} from N({mean_val:.2f}, {std_val:.2f})\")\n",
    "            else:\n",
    "                # Fallback to mean imputation if no stats available\n",
    "                mean_val = df_processed[col].dropna().mean()\n",
    "                if pd.isna(mean_val):  # If all values are NA (unlikely, but possible)\n",
    "                    mean_val = 0\n",
    "                df_processed[col].fillna(mean_val, inplace=True)\n",
    "                print(f\"Used mean imputation for {col}: {mean_val:.2f}\")\n",
    "    \n",
    "    print(f\"Missing values after sampling: {df_processed[feature_columns].isna().sum().sum()}\")\n",
    "\n",
    "    # Final check - any remaining nulls get mean imputation\n",
    "    print(\"Imputing any remaining null values with column means...\")\n",
    "    for col in feature_columns:\n",
    "        if df_processed[col].isna().sum() > 0:\n",
    "            mean_val = df_processed[col].dropna().mean()\n",
    "            if pd.isna(mean_val):  # If all values are NA (unlikely, but possible)\n",
    "                mean_val = 0\n",
    "            df_processed[col].fillna(mean_val, inplace=True)\n",
    "    \n",
    "    print(f\"Missing values after complete imputation: {df_processed[feature_columns].isna().sum().sum()}\")\n",
    "    \n",
    "    # Final check to catch any remaining missing values\n",
    "    remaining_nulls = df_processed.isna().sum()\n",
    "    if remaining_nulls.sum() > 0:\n",
    "        print(\"WARNING: Remaining null values detected!\")\n",
    "        print(remaining_nulls[remaining_nulls > 0])\n",
    "        print(\"Performing final fill with zeros...\")\n",
    "        df_processed.fillna(0, inplace=True)  # Final safety measure\n",
    "    \n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def fit_enhanced_preprocessor(train_data, output_folder='preprocessed_data'):\n",
    "    \"\"\"\n",
    "    Fit preprocessing parameters with enhanced scaling on training data.\n",
    "    Uses log transform, minmax, or robust scaling based on feature distributions.\n",
    "    Also calculates normal distribution parameters for imputation.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pickle\n",
    "    from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "    \n",
    "    # Read the training data if path is provided\n",
    "    if isinstance(train_data, str):\n",
    "        df = pd.read_csv(train_data)\n",
    "    else:\n",
    "        df = train_data.copy()\n",
    "    \n",
    "    print(f\"Training data shape: {df.shape}\")\n",
    "    print(f\"Missing values before preprocessing: {df.isna().sum().sum()}\")\n",
    "    \n",
    "    # Define columns that we don't want to impute or scale\n",
    "    id_columns = ['PatientID', 'Hours']\n",
    "    categorical_columns = ['Gender', 'ICUType']\n",
    "    \n",
    "    # Get modes for categorical columns\n",
    "    categorical_modes = {}\n",
    "    for col in categorical_columns:\n",
    "        if col in df.columns:\n",
    "            categorical_modes[col] = df[col].mode()[0]\n",
    "    \n",
    "    # Group features by scaling type\n",
    "    log_transform_features = ['Glucose', 'BUN', 'Creatinine', 'AST', 'ALT', 'ALP', 'Bilirubin', \n",
    "                             'Troponin', 'Lactate', 'PaO2', 'Urine', 'WBC', 'Platelets']\n",
    "    \n",
    "    minmax_features = ['pH', 'FiO2', 'O2Sat', 'GCS', 'Age']\n",
    "    \n",
    "    # Columns that need imputation and scaling\n",
    "    feature_columns = [col for col in df.columns if col not in id_columns + categorical_columns]\n",
    "    \n",
    "    # All remaining features get robust scaling\n",
    "    robust_scale_features = [col for col in feature_columns \n",
    "                            if col not in log_transform_features + minmax_features]\n",
    "    \n",
    "    # Filter to only include features present in the dataset\n",
    "    log_transform_features = [col for col in log_transform_features if col in feature_columns]\n",
    "    minmax_features = [col for col in minmax_features if col in feature_columns]\n",
    "    robust_scale_features = [col for col in robust_scale_features if col in feature_columns]\n",
    "    \n",
    "    print(f\"Log transform features: {len(log_transform_features)}\")\n",
    "    print(f\"MinMax features: {len(minmax_features)}\")\n",
    "    print(f\"Robust scale features: {len(robust_scale_features)}\")\n",
    "    \n",
    "    # Helper function for preprocessing without scaling for fitting\n",
    "    def preprocess_for_stats(df, feature_columns, id_columns, categorical_columns, categorical_modes):\n",
    "        \"\"\"Simple preprocessing to calculate distribution statistics\"\"\"\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Apply categorical imputation\n",
    "        for col in categorical_columns:\n",
    "            if col in df_processed.columns and df_processed[col].isna().sum() > 0:\n",
    "                df_processed[col].fillna(categorical_modes.get(col, df_processed[col].mode()[0]), inplace=True)\n",
    "        \n",
    "        # Apply forward and backward filling to get complete data for stats calculation\n",
    "        for patient_id in df_processed['PatientID'].unique():\n",
    "            patient_mask = df_processed['PatientID'] == patient_id\n",
    "            df_processed.loc[patient_mask, feature_columns] = (\n",
    "                df_processed.loc[patient_mask, feature_columns].ffill().bfill()\n",
    "            )\n",
    "        \n",
    "        # Apply mean imputation for any remaining nulls\n",
    "        for col in feature_columns:\n",
    "            mean_val = df_processed[col].dropna().mean()\n",
    "            if pd.isna(mean_val):\n",
    "                mean_val = 0\n",
    "            df_processed[col].fillna(mean_val, inplace=True)\n",
    "        \n",
    "        return df_processed\n",
    "    \n",
    "    # Get a complete dataset for calculating statistics\n",
    "    df_for_stats = preprocess_for_stats(\n",
    "        df,\n",
    "        feature_columns,\n",
    "        id_columns,\n",
    "        categorical_columns,\n",
    "        categorical_modes\n",
    "    )\n",
    "    \n",
    "    # Calculate statistics from complete data\n",
    "    stats = {}\n",
    "    for col in feature_columns:\n",
    "        values = df_for_stats[col]\n",
    "        stats[col] = {\n",
    "            'mean': values.mean(),\n",
    "            'median': values.median(),\n",
    "            'min': values.min(),\n",
    "            'max': values.max(),\n",
    "            'std': values.std(),\n",
    "            'q1': values.quantile(0.25),\n",
    "            'q3': values.quantile(0.75)\n",
    "        }\n",
    "    \n",
    "    # Now preprocess training data using the normal distribution sampling\n",
    "    df_train_processed = improved_preprocess_without_scaling(\n",
    "        df,\n",
    "        feature_columns,\n",
    "        id_columns,\n",
    "        categorical_columns,\n",
    "        categorical_modes,\n",
    "        stats=stats\n",
    "    )\n",
    "    \n",
    "    # Fit different scalers\n",
    "    # Robust scaler for features with outliers\n",
    "    robust_scaler = RobustScaler()\n",
    "    if robust_scale_features:\n",
    "        robust_scaler.fit(df_train_processed[robust_scale_features])\n",
    "    \n",
    "    # MinMax scaler for bounded variables\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    if minmax_features:\n",
    "        minmax_scaler.fit(df_train_processed[minmax_features])\n",
    "    \n",
    "    # Log transformation doesn't need fitting, but we'll store the epsilon value\n",
    "    log_epsilon = 1e-6\n",
    "    \n",
    "    # Save the scalers and parameters\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Create a scaling info dictionary\n",
    "    scaling_info = {\n",
    "        'robust_features': robust_scale_features,\n",
    "        'minmax_features': minmax_features,\n",
    "        'log_features': log_transform_features,\n",
    "        'feature_columns': feature_columns,\n",
    "        'log_epsilon': log_epsilon\n",
    "    }\n",
    "    \n",
    "    # Save all objects\n",
    "    pickle.dump(robust_scaler, open(os.path.join(output_folder, 'robust_scaler.pkl'), 'wb'))\n",
    "    pickle.dump(minmax_scaler, open(os.path.join(output_folder, 'minmax_scaler.pkl'), 'wb'))\n",
    "    pickle.dump(scaling_info, open(os.path.join(output_folder, 'scaling_info.pkl'), 'wb'))\n",
    "    pickle.dump(stats, open(os.path.join(output_folder, 'improved_imputation_stats.pkl'), 'wb'))\n",
    "    pickle.dump(categorical_modes, open(os.path.join(output_folder, 'improved_categorical_modes.pkl'), 'wb'))\n",
    "    \n",
    "    print(f\"Enhanced preprocessing parameters saved to: {output_folder}\")\n",
    "    \n",
    "    return {\n",
    "        'robust_scaler': robust_scaler,\n",
    "        'minmax_scaler': minmax_scaler,\n",
    "        'scaling_info': scaling_info,\n",
    "        'stats': stats,\n",
    "        'categorical_modes': categorical_modes\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def apply_enhanced_scaling(df, preprocessing_params):\n",
    "    \"\"\"\n",
    "    Apply enhanced scaling to the preprocessed dataframe using log, minmax, or robust scaling\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    df_scaled = df.copy()\n",
    "    \n",
    "    # Extract parameters\n",
    "    robust_scaler = preprocessing_params['robust_scaler']\n",
    "    minmax_scaler = preprocessing_params['minmax_scaler']\n",
    "    scaling_info = preprocessing_params['scaling_info']\n",
    "    \n",
    "    # Apply log transformation first\n",
    "    for col in scaling_info['log_features']:\n",
    "        if col in df.columns:\n",
    "            # Add small epsilon to avoid log(0)\n",
    "            epsilon = scaling_info.get('log_epsilon', 1e-6)\n",
    "            # Clip negative values to epsilon\n",
    "            df_scaled[col] = np.log1p(df[col].clip(lower=epsilon))\n",
    "    \n",
    "    # Apply robust scaling\n",
    "    if scaling_info['robust_features']:\n",
    "        df_scaled[scaling_info['robust_features']] = robust_scaler.transform(\n",
    "            df[scaling_info['robust_features']]\n",
    "        )\n",
    "    \n",
    "    # Apply minmax scaling\n",
    "    if scaling_info['minmax_features']:\n",
    "        df_scaled[scaling_info['minmax_features']] = minmax_scaler.transform(\n",
    "            df[scaling_info['minmax_features']]\n",
    "        )\n",
    "    \n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def enhanced_preprocess_patient_data(data, preprocessing_params=None, output_folder='preprocessed_data', \n",
    "                                    output_filename=None, is_training=False, random_state=42):\n",
    "    \"\"\"\n",
    "    Preprocess patient data with enhanced method using log, minmax, or robust scaling\n",
    "    based on feature distributions. Now uses normal distribution sampling instead of backward filling.\n",
    "    \"\"\"\n",
    " \n",
    "    \n",
    "    # Read the data if path is provided\n",
    "    if isinstance(data, str):\n",
    "        df = pd.read_csv(data)\n",
    "        # Generate a filename if needed\n",
    "        if output_filename is None:\n",
    "            base_filename = os.path.basename(data)\n",
    "            dataset_type = \"train\" if is_training else \"test_or_val\"\n",
    "            output_filename = f\"enhanced_{dataset_type}_{base_filename}\"\n",
    "    else:\n",
    "        df = data.copy()\n",
    "        # Default filename if needed\n",
    "        if output_filename is None:\n",
    "            dataset_type = \"train\" if is_training else \"test_or_val\"\n",
    "            output_filename = f\"{dataset_type}_enhanced_scaled_data.csv\"\n",
    "    \n",
    "    print(f\"Data shape: {df.shape}\")\n",
    "    print(f\"Missing values before preprocessing: {df.isna().sum().sum()}\")\n",
    "    \n",
    "    # Define columns that we don't want to impute or scale\n",
    "    id_columns = ['PatientID', 'Hours']\n",
    "    categorical_columns = ['Gender', 'ICUType']\n",
    "    \n",
    "    # If training data and no preprocessing params provided, fit new ones\n",
    "    if is_training and preprocessing_params is None:\n",
    "        preprocessing_params = fit_enhanced_preprocessor(df, output_folder)\n",
    "        scaling_info = preprocessing_params['scaling_info']\n",
    "        feature_columns = scaling_info['feature_columns']\n",
    "        stats = preprocessing_params['stats']\n",
    "        categorical_modes = preprocessing_params['categorical_modes']\n",
    "    elif preprocessing_params is None:\n",
    "        # Load preprocessing parameters\n",
    "        robust_scaler = pickle.load(open(os.path.join(output_folder, 'robust_scaler.pkl'), 'rb'))\n",
    "        minmax_scaler = pickle.load(open(os.path.join(output_folder, 'minmax_scaler.pkl'), 'rb'))\n",
    "        scaling_info = pickle.load(open(os.path.join(output_folder, 'scaling_info.pkl'), 'rb'))\n",
    "        stats = pickle.load(open(os.path.join(output_folder, 'improved_imputation_stats.pkl'), 'rb'))\n",
    "        categorical_modes = pickle.load(open(os.path.join(output_folder, 'improved_categorical_modes.pkl'), 'rb'))\n",
    "        \n",
    "        preprocessing_params = {\n",
    "            'robust_scaler': robust_scaler,\n",
    "            'minmax_scaler': minmax_scaler,\n",
    "            'scaling_info': scaling_info,\n",
    "            'stats': stats,\n",
    "            'categorical_modes': categorical_modes\n",
    "        }\n",
    "        feature_columns = scaling_info['feature_columns']\n",
    "    else:\n",
    "        scaling_info = preprocessing_params['scaling_info']\n",
    "        feature_columns = scaling_info['feature_columns']\n",
    "        stats = preprocessing_params['stats']\n",
    "        categorical_modes = preprocessing_params['categorical_modes']\n",
    "    \n",
    "    # Preprocess without scaling using normal distribution sampling\n",
    "    df_processed = improved_preprocess_without_scaling(\n",
    "        df, \n",
    "        feature_columns, \n",
    "        id_columns, \n",
    "        categorical_columns, \n",
    "        preprocessing_params['categorical_modes'],\n",
    "        stats=preprocessing_params['stats'],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"Missing values after imputation: {df_processed[feature_columns].isna().sum().sum()}\")\n",
    "    \n",
    "    # Apply enhanced scaling\n",
    "    print(\"Applying enhanced feature scaling...\")\n",
    "    df_scaled = apply_enhanced_scaling(df_processed, preprocessing_params)\n",
    "    \n",
    "    # Save the preprocessed data if output folder is provided\n",
    "    output_path = None\n",
    "    if output_folder:\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "            \n",
    "        output_path = os.path.join(output_folder, output_filename)\n",
    "        df_scaled.to_csv(output_path, index=False)\n",
    "        print(f\"Enhanced preprocessed data saved to: {output_path}\")\n",
    "    \n",
    "    # Print some statistics about the preprocessing\n",
    "    print(\"\\nPreprocessing Summary:\")\n",
    "    print(f\"Total rows: {len(df_scaled)}\")\n",
    "    print(f\"Total features: {len(feature_columns)}\")\n",
    "    print(f\"Features with log transform: {len(scaling_info['log_features'])}\")\n",
    "    print(f\"Features with robust scaling: {len(scaling_info['robust_features'])}\")\n",
    "    print(f\"Features with minmax scaling: {len(scaling_info['minmax_features'])}\")\n",
    "    print(f\"Missing values in original data: {df.isna().sum().sum()}\")\n",
    "    print(f\"Missing values in preprocessed data: {df_scaled.isna().sum().sum()}\")\n",
    "    \n",
    "    if is_training:\n",
    "        return df_scaled, output_path, preprocessing_params\n",
    "    else:\n",
    "        return df_scaled, output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_356242/1759002329.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_processed[col].fillna(categorical_modes[col], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying forward filling...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m feature_columns = scaling_info[\u001b[33m'\u001b[39m\u001b[33mfeature_columns\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Apply only imputation, not scaling\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m train_imputed_only = \u001b[43mimproved_preprocess_without_scaling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcategorical_modes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m val_imputed_only = improved_preprocess_without_scaling(\n\u001b[32m     29\u001b[39m     val_data, feature_columns, id_columns, categorical_columns, \n\u001b[32m     30\u001b[39m     categorical_modes, stats=stats)\n\u001b[32m     32\u001b[39m test_imputed_only = improved_preprocess_without_scaling(\n\u001b[32m     33\u001b[39m     test_data, feature_columns, id_columns, categorical_columns, \n\u001b[32m     34\u001b[39m     categorical_modes, stats=stats)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mimproved_preprocess_without_scaling\u001b[39m\u001b[34m(df, feature_columns, id_columns, categorical_columns, categorical_modes, stats, random_state)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mApplying forward filling...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m patient_id \u001b[38;5;129;01min\u001b[39;00m df_processed[\u001b[33m'\u001b[39m\u001b[33mPatientID\u001b[39m\u001b[33m'\u001b[39m].unique():\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     patient_mask = \u001b[43mdf_processed\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPatientID\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatient_id\u001b[49m\n\u001b[32m     28\u001b[39m     df_processed.loc[patient_mask, feature_columns] = df_processed.loc[patient_mask, feature_columns].ffill()\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing values after forward filling: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_processed[feature_columns].isna().sum().sum()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/pandas/core/ops/common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/pandas/core/arraylike.py:40\u001b[39m, in \u001b[36mOpsMixin.__eq__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__eq__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/pandas/core/series.py:6119\u001b[39m, in \u001b[36mSeries._cmp_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   6116\u001b[39m lvalues = \u001b[38;5;28mself\u001b[39m._values\n\u001b[32m   6117\u001b[39m rvalues = extract_array(other, extract_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m6119\u001b[39m res_values = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6121\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._construct_result(res_values, name=res_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:347\u001b[39m, in \u001b[36mcomparison_op\u001b[39m\u001b[34m(left, right, op)\u001b[39m\n\u001b[32m    344\u001b[39m     res_values = comp_method_OBJECT_ARRAY(op, lvalues, rvalues)\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m     res_values = \u001b[43m_na_arithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_cmp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:218\u001b[39m, in \u001b[36m_na_arithmetic_op\u001b[39m\u001b[34m(left, right, op, is_cmp)\u001b[39m\n\u001b[32m    215\u001b[39m     func = partial(expressions.evaluate, op)\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    221\u001b[39m         left.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(right, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mobject\u001b[39m\n\u001b[32m    222\u001b[39m     ):\n\u001b[32m   (...)\u001b[39m\u001b[32m    225\u001b[39m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[32m    226\u001b[39m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/pandas/core/computation/expressions.py:242\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(op, a, b, use_numexpr)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m op_str \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_numexpr:\n\u001b[32m    241\u001b[39m         \u001b[38;5;66;03m# error: \"None\" not callable\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/pandas/core/computation/expressions.py:73\u001b[39m, in \u001b[36m_evaluate_standard\u001b[39m\u001b[34m(op, op_str, a, b)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _TEST_MODE:\n\u001b[32m     72\u001b[39m     _store_test_result(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_data_path = \"/home/taekim/csv_data/patient_set-a.csv\"  # Training set\n",
    "val_data_path = \"/home/taekim/csv_data/patient_set-b.csv\"    # Validation set\n",
    "test_data_path = \"/home/taekim/csv_data/patient_set-c.csv\"   # Test set\n",
    "output_folder = \"enhanced_preprocessed_data\"\n",
    "# Reload your data\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "val_data = pd.read_csv(val_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "# Load preprocessing parameters\n",
    "with open(os.path.join(output_folder, 'scaling_info.pkl'), 'rb') as f:\n",
    "    scaling_info = pickle.load(f)\n",
    "with open(os.path.join(output_folder, 'improved_imputation_stats.pkl'), 'rb') as f:\n",
    "    stats = pickle.load(f)\n",
    "with open(os.path.join(output_folder, 'improved_categorical_modes.pkl'), 'rb') as f:\n",
    "    categorical_modes = pickle.load(f)\n",
    "\n",
    "# Define columns\n",
    "id_columns = ['PatientID', 'Hours']\n",
    "categorical_columns = ['Gender', 'ICUType']\n",
    "feature_columns = scaling_info['feature_columns']\n",
    "\n",
    "# Apply only imputation, not scaling\n",
    "train_imputed_only = improved_preprocess_without_scaling(\n",
    "    train_data, feature_columns, id_columns, categorical_columns, \n",
    "    categorical_modes, stats=stats)\n",
    "\n",
    "val_imputed_only = improved_preprocess_without_scaling(\n",
    "    val_data, feature_columns, id_columns, categorical_columns, \n",
    "    categorical_modes, stats=stats)\n",
    "\n",
    "test_imputed_only = improved_preprocess_without_scaling(\n",
    "    test_data, feature_columns, id_columns, categorical_columns, \n",
    "    categorical_modes, stats=stats)\n",
    "\n",
    "# Save to new files\n",
    "train_imputed_only.to_csv(os.path.join(output_folder, 'imputed_only_set-a.csv'), index=False)\n",
    "val_imputed_only.to_csv(os.path.join(output_folder, 'imputed_only_set-b.csv'), index=False)\n",
    "test_imputed_only.to_csv(os.path.join(output_folder, 'imputed_only_set-c.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (196000, 43)\n",
      "Missing values before preprocessing: 5904543\n",
      "Training data shape: (196000, 43)\n",
      "Missing values before preprocessing: 5904543\n",
      "Log transform features: 12\n",
      "MinMax features: 4\n",
      "Robust scale features: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_333586/2423481867.py:62: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_processed[col].fillna(categorical_modes.get(col, df_processed[col].mode()[0]), inplace=True)\n",
      "/tmp/ipykernel_333586/2423481867.py:76: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_processed[col].fillna(mean_val, inplace=True)\n",
      "/tmp/ipykernel_333586/1759002329.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_processed[col].fillna(categorical_modes[col], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying forward filling...\n",
      "Missing values after forward filling: 2784218\n",
      "Sampling from normal distribution for remaining NAs...\n",
      "Sampled 27679 values for BUN from N(25.47, 20.98)\n",
      "Sampled 27624 values for Creatinine from N(1.37, 1.43)\n",
      "Sampled 75136 values for DiasABP from N(59.06, 12.85)\n",
      "Sampled 79351 values for FiO2 from N(0.55, 0.16)\n",
      "Sampled 14130 values for GCS from N(11.59, 4.00)\n",
      "Sampled 37204 values for Glucose from N(136.76, 55.66)\n",
      "Sampled 29433 values for HCO3 from N(23.59, 4.34)\n",
      "Sampled 25931 values for HCT from N(31.62, 5.06)\n",
      "Sampled 11353 values for HR from N(86.72, 17.64)\n",
      "Sampled 92806 values for Height from N(169.79, 14.64)\n",
      "Sampled 34689 values for K from N(4.14, 0.61)\n",
      "Sampled 75678 values for MAP from N(81.41, 16.95)\n",
      "Sampled 85403 values for MechVent from N(1.00, 0.00)\n",
      "Sampled 36436 values for Mg from N(2.02, 0.40)\n",
      "Sampled 48976 values for NIDiasABP from N(57.19, 14.05)\n",
      "Sampled 49125 values for NIMAP from N(76.12, 14.26)\n",
      "Sampled 48557 values for NISysABP from N(116.65, 23.43)\n",
      "Sampled 34048 values for Na from N(138.91, 4.39)\n",
      "Sampled 65952 values for PaCO2 from N(40.12, 7.65)\n",
      "Sampled 66057 values for PaO2 from N(136.95, 66.87)\n",
      "Sampled 27546 values for Platelets from N(207.28, 106.12)\n",
      "Sampled 129529 values for SaO2 from N(96.61, 2.55)\n",
      "Sampled 75135 values for SysABP from N(117.88, 23.72)\n",
      "Sampled 13060 values for Temp from N(36.96, 1.37)\n",
      "Sampled 16957 values for Urine from N(147.69, 223.38)\n",
      "Sampled 31162 values for WBC from N(12.60, 6.97)\n",
      "Sampled 14540 values for Weight from N(82.46, 23.09)\n",
      "Sampled 65302 values for pH from N(7.53, 5.99)\n",
      "Sampled 107968 values for Lactate from N(2.20, 1.29)\n",
      "Sampled 131480 values for ALP from N(104.60, 69.77)\n",
      "Sampled 130242 values for ALT from N(170.68, 449.10)\n",
      "Sampled 130123 values for AST from N(241.73, 666.27)\n",
      "Sampled 137260 values for Albumin from N(2.98, 0.41)\n",
      "Sampled 131062 values for Bilirubin from N(1.89, 2.91)\n",
      "Sampled 184789 values for Cholesterol from N(156.50, 12.78)\n",
      "Sampled 160271 values for TroponinT from N(1.07, 1.28)\n",
      "Sampled 144198 values for RespRate from N(19.67, 2.91)\n",
      "Sampled 188026 values for TroponinI from N(6.99, 2.28)\n",
      "Missing values after sampling: 0\n",
      "Imputing any remaining null values with column means...\n",
      "Missing values after complete imputation: 0\n",
      "Enhanced preprocessing parameters saved to: enhanced_preprocessed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_333586/1759002329.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_processed[col].fillna(categorical_modes[col], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying forward filling...\n",
      "Missing values after forward filling: 2784218\n",
      "Sampling from normal distribution for remaining NAs...\n",
      "Sampled 27679 values for BUN from N(25.47, 20.98)\n",
      "Sampled 27624 values for Creatinine from N(1.37, 1.43)\n",
      "Sampled 75136 values for DiasABP from N(59.06, 12.85)\n",
      "Sampled 79351 values for FiO2 from N(0.55, 0.16)\n",
      "Sampled 14130 values for GCS from N(11.59, 4.00)\n",
      "Sampled 37204 values for Glucose from N(136.76, 55.66)\n",
      "Sampled 29433 values for HCO3 from N(23.59, 4.34)\n",
      "Sampled 25931 values for HCT from N(31.62, 5.06)\n",
      "Sampled 11353 values for HR from N(86.72, 17.64)\n",
      "Sampled 92806 values for Height from N(169.79, 14.64)\n",
      "Sampled 34689 values for K from N(4.14, 0.61)\n",
      "Sampled 75678 values for MAP from N(81.41, 16.95)\n",
      "Sampled 85403 values for MechVent from N(1.00, 0.00)\n",
      "Sampled 36436 values for Mg from N(2.02, 0.40)\n",
      "Sampled 48976 values for NIDiasABP from N(57.19, 14.05)\n",
      "Sampled 49125 values for NIMAP from N(76.12, 14.26)\n",
      "Sampled 48557 values for NISysABP from N(116.65, 23.43)\n",
      "Sampled 34048 values for Na from N(138.91, 4.39)\n",
      "Sampled 65952 values for PaCO2 from N(40.12, 7.65)\n",
      "Sampled 66057 values for PaO2 from N(136.95, 66.87)\n",
      "Sampled 27546 values for Platelets from N(207.28, 106.12)\n",
      "Sampled 129529 values for SaO2 from N(96.61, 2.55)\n",
      "Sampled 75135 values for SysABP from N(117.88, 23.72)\n",
      "Sampled 13060 values for Temp from N(36.96, 1.37)\n",
      "Sampled 16957 values for Urine from N(147.69, 223.38)\n",
      "Sampled 31162 values for WBC from N(12.60, 6.97)\n",
      "Sampled 14540 values for Weight from N(82.46, 23.09)\n",
      "Sampled 65302 values for pH from N(7.53, 5.99)\n",
      "Sampled 107968 values for Lactate from N(2.20, 1.29)\n",
      "Sampled 131480 values for ALP from N(104.60, 69.77)\n",
      "Sampled 130242 values for ALT from N(170.68, 449.10)\n",
      "Sampled 130123 values for AST from N(241.73, 666.27)\n",
      "Sampled 137260 values for Albumin from N(2.98, 0.41)\n",
      "Sampled 131062 values for Bilirubin from N(1.89, 2.91)\n",
      "Sampled 184789 values for Cholesterol from N(156.50, 12.78)\n",
      "Sampled 160271 values for TroponinT from N(1.07, 1.28)\n",
      "Sampled 144198 values for RespRate from N(19.67, 2.91)\n",
      "Sampled 188026 values for TroponinI from N(6.99, 2.28)\n",
      "Missing values after sampling: 0\n",
      "Imputing any remaining null values with column means...\n",
      "Missing values after complete imputation: 0\n",
      "Missing values after imputation: 0\n",
      "Applying enhanced feature scaling...\n",
      "Enhanced preprocessed data saved to: enhanced_preprocessed_data/enhanced_sampled_set-a.csv\n",
      "\n",
      "Preprocessing Summary:\n",
      "Total rows: 196000\n",
      "Total features: 39\n",
      "Features with log transform: 12\n",
      "Features with robust scaling: 23\n",
      "Features with minmax scaling: 4\n",
      "Missing values in original data: 5904543\n",
      "Missing values in preprocessed data: 0\n",
      "Data shape: (196000, 43)\n",
      "Missing values before preprocessing: 5903300\n",
      "Applying forward filling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_333586/1759002329.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_processed[col].fillna(categorical_modes[col], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after forward filling: 2766959\n",
      "Sampling from normal distribution for remaining NAs...\n",
      "Sampled 27312 values for BUN from N(25.47, 20.98)\n",
      "Sampled 27191 values for Creatinine from N(1.37, 1.43)\n",
      "Sampled 73825 values for DiasABP from N(59.06, 12.85)\n",
      "Sampled 80651 values for FiO2 from N(0.55, 0.16)\n",
      "Sampled 14003 values for GCS from N(11.59, 4.00)\n",
      "Sampled 35655 values for Glucose from N(136.76, 55.66)\n",
      "Sampled 28794 values for HCO3 from N(23.59, 4.34)\n",
      "Sampled 25161 values for HCT from N(31.62, 5.06)\n",
      "Sampled 11226 values for HR from N(86.72, 17.64)\n",
      "Sampled 94570 values for Height from N(169.79, 14.64)\n",
      "Sampled 33083 values for K from N(4.14, 0.61)\n",
      "Sampled 74225 values for MAP from N(81.41, 16.95)\n",
      "Sampled 86873 values for MechVent from N(1.00, 0.00)\n",
      "Sampled 36164 values for Mg from N(2.02, 0.40)\n",
      "Sampled 47023 values for NIDiasABP from N(57.19, 14.05)\n",
      "Sampled 47066 values for NIMAP from N(76.12, 14.26)\n",
      "Sampled 46354 values for NISysABP from N(116.65, 23.43)\n",
      "Sampled 32848 values for Na from N(138.91, 4.39)\n",
      "Sampled 67558 values for PaCO2 from N(40.12, 7.65)\n",
      "Sampled 67741 values for PaO2 from N(136.95, 66.87)\n",
      "Sampled 26931 values for Platelets from N(207.28, 106.12)\n",
      "Sampled 131203 values for SaO2 from N(96.61, 2.55)\n",
      "Sampled 73818 values for SysABP from N(117.88, 23.72)\n",
      "Sampled 12988 values for Temp from N(36.96, 1.37)\n",
      "Sampled 16074 values for Urine from N(147.69, 223.38)\n",
      "Sampled 30741 values for WBC from N(12.60, 6.97)\n",
      "Sampled 15709 values for Weight from N(82.46, 23.09)\n",
      "Sampled 66655 values for pH from N(7.53, 5.99)\n",
      "Sampled 106575 values for Lactate from N(2.20, 1.29)\n",
      "Sampled 130945 values for ALP from N(104.60, 69.77)\n",
      "Sampled 129182 values for ALT from N(170.68, 449.10)\n",
      "Sampled 129096 values for AST from N(241.73, 666.27)\n",
      "Sampled 137107 values for Albumin from N(2.98, 0.41)\n",
      "Sampled 129710 values for Bilirubin from N(1.89, 2.91)\n",
      "Sampled 183225 values for Cholesterol from N(156.50, 12.78)\n",
      "Sampled 157594 values for TroponinT from N(1.07, 1.28)\n",
      "Sampled 142961 values for RespRate from N(19.67, 2.91)\n",
      "Sampled 189122 values for TroponinI from N(6.99, 2.28)\n",
      "Missing values after sampling: 0\n",
      "Imputing any remaining null values with column means...\n",
      "Missing values after complete imputation: 0\n",
      "Missing values after imputation: 0\n",
      "Applying enhanced feature scaling...\n",
      "Enhanced preprocessed data saved to: enhanced_preprocessed_data/enhanced_sampled_set-b.csv\n",
      "\n",
      "Preprocessing Summary:\n",
      "Total rows: 196000\n",
      "Total features: 39\n",
      "Features with log transform: 12\n",
      "Features with robust scaling: 23\n",
      "Features with minmax scaling: 4\n",
      "Missing values in original data: 5903300\n",
      "Missing values in preprocessed data: 0\n",
      "Data shape: (196000, 43)\n",
      "Missing values before preprocessing: 5906893\n",
      "Applying forward filling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_333586/1759002329.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_processed[col].fillna(categorical_modes[col], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after forward filling: 2773743\n",
      "Sampling from normal distribution for remaining NAs...\n",
      "Sampled 28042 values for BUN from N(25.47, 20.98)\n",
      "Sampled 28010 values for Creatinine from N(1.37, 1.43)\n",
      "Sampled 73354 values for DiasABP from N(59.06, 12.85)\n",
      "Sampled 80787 values for FiO2 from N(0.55, 0.16)\n",
      "Sampled 13659 values for GCS from N(11.59, 4.00)\n",
      "Sampled 37084 values for Glucose from N(136.76, 55.66)\n",
      "Sampled 29791 values for HCO3 from N(23.59, 4.34)\n",
      "Sampled 26099 values for HCT from N(31.62, 5.06)\n",
      "Sampled 11037 values for HR from N(86.72, 17.64)\n",
      "Sampled 93198 values for Height from N(169.79, 14.64)\n",
      "Sampled 34274 values for K from N(4.14, 0.61)\n",
      "Sampled 73800 values for MAP from N(81.41, 16.95)\n",
      "Sampled 85642 values for MechVent from N(1.00, 0.00)\n",
      "Sampled 37276 values for Mg from N(2.02, 0.40)\n",
      "Sampled 51046 values for NIDiasABP from N(57.19, 14.05)\n",
      "Sampled 51184 values for NIMAP from N(76.12, 14.26)\n",
      "Sampled 50463 values for NISysABP from N(116.65, 23.43)\n",
      "Sampled 34337 values for Na from N(138.91, 4.39)\n",
      "Sampled 65893 values for PaCO2 from N(40.12, 7.65)\n",
      "Sampled 66089 values for PaO2 from N(136.95, 66.87)\n",
      "Sampled 27557 values for Platelets from N(207.28, 106.12)\n",
      "Sampled 127323 values for SaO2 from N(96.61, 2.55)\n",
      "Sampled 73348 values for SysABP from N(117.88, 23.72)\n",
      "Sampled 12796 values for Temp from N(36.96, 1.37)\n",
      "Sampled 16224 values for Urine from N(147.69, 223.38)\n",
      "Sampled 31723 values for WBC from N(12.60, 6.97)\n",
      "Sampled 14778 values for Weight from N(82.46, 23.09)\n",
      "Sampled 65020 values for pH from N(7.53, 5.99)\n",
      "Sampled 107180 values for Lactate from N(2.20, 1.29)\n",
      "Sampled 128760 values for ALP from N(104.60, 69.77)\n",
      "Sampled 127131 values for ALT from N(170.68, 449.10)\n",
      "Sampled 127348 values for AST from N(241.73, 666.27)\n",
      "Sampled 136151 values for Albumin from N(2.98, 0.41)\n",
      "Sampled 127357 values for Bilirubin from N(1.89, 2.91)\n",
      "Sampled 184593 values for Cholesterol from N(156.50, 12.78)\n",
      "Sampled 161975 values for TroponinT from N(1.07, 1.28)\n",
      "Sampled 144384 values for RespRate from N(19.67, 2.91)\n",
      "Sampled 189030 values for TroponinI from N(6.99, 2.28)\n",
      "Missing values after sampling: 0\n",
      "Imputing any remaining null values with column means...\n",
      "Missing values after complete imputation: 0\n",
      "Missing values after imputation: 0\n",
      "Applying enhanced feature scaling...\n",
      "Enhanced preprocessed data saved to: enhanced_preprocessed_data/enhanced_sampled_set-c.csv\n",
      "\n",
      "Preprocessing Summary:\n",
      "Total rows: 196000\n",
      "Total features: 39\n",
      "Features with log transform: 12\n",
      "Features with robust scaling: 23\n",
      "Features with minmax scaling: 4\n",
      "Missing values in original data: 5906893\n",
      "Missing values in preprocessed data: 0\n",
      "\n",
      "All datasets have been preprocessed with enhanced method:\n",
      "Training data: enhanced_preprocessed_data/enhanced_sampled_set-a.csv\n",
      "Validation data: enhanced_preprocessed_data/enhanced_sampled_set-b.csv\n",
      "Test data: enhanced_preprocessed_data/enhanced_sampled_set-c.csv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "\n",
    "# Define data paths\n",
    "train_data_path = \"/home/taekim/csv_data/patient_set-a.csv\"  # Training set\n",
    "val_data_path = \"/home/taekim/csv_data/patient_set-b.csv\"    # Validation set\n",
    "test_data_path = \"/home/taekim/csv_data/patient_set-c.csv\"   # Test set\n",
    "output_folder = \"enhanced_preprocessed_data\"\n",
    "\n",
    "# Process training data and get preprocessing parameters\n",
    "train_enhanced, train_enhanced_path, preprocessing_params = enhanced_preprocess_patient_data(\n",
    "    train_data_path,\n",
    "    output_folder=output_folder,\n",
    "    output_filename=\"enhanced_sampled_set-a.csv\",\n",
    "    is_training=True\n",
    ")\n",
    "\n",
    "# Process validation data using same parameters\n",
    "val_enhanced, val_enhanced_path = enhanced_preprocess_patient_data(\n",
    "    val_data_path,\n",
    "    preprocessing_params=preprocessing_params,\n",
    "    output_folder=output_folder,\n",
    "    output_filename=\"enhanced_sampled_set-b.csv\",\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "# Process test data using same parameters\n",
    "test_enhanced, test_enhanced_path = enhanced_preprocess_patient_data(\n",
    "    test_data_path,\n",
    "    preprocessing_params=preprocessing_params,\n",
    "    output_folder=output_folder,\n",
    "    output_filename=\"enhanced_sampled_set-c.csv\",\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "print(\"\\nAll datasets have been preprocessed with enhanced method:\")\n",
    "print(f\"Training data: {train_enhanced_path}\")\n",
    "print(f\"Validation data: {val_enhanced_path}\")\n",
    "print(f\"Test data: {test_enhanced_path}\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
