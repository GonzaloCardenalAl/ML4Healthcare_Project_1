{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def load_and_convert_grid_to_triplets(grid_data_path, output_file='grid_triplets.csv', \n",
    "                                     preprocess_params_dir='enhanced_preprocessed_data'):\n",
    "    \"\"\"\n",
    "    Alternative method: Load existing grid data and convert it to triplets format.\n",
    "    Useful if you already have the time-grid data from previous steps.\n",
    "    \n",
    "    Args:\n",
    "        grid_data_path: Path to the patient grid data (from previous processing)\n",
    "        output_file: Name of the output file to save triplets\n",
    "        preprocess_params_dir: Directory containing preprocessing parameters\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame containing triplets\n",
    "    \"\"\"\n",
    "    print(f\"Loading grid data from {grid_data_path} and converting to triplets\")\n",
    "    \n",
    "    # Load grid data\n",
    "    grid_df = pd.read_csv(grid_data_path)\n",
    "    \n",
    "    # Load preprocessing parameters\n",
    "    try:\n",
    "        robust_scaler = pickle.load(open(os.path.join(preprocess_params_dir, 'robust_scaler.pkl'), 'rb'))\n",
    "        minmax_scaler = pickle.load(open(os.path.join(preprocess_params_dir, 'minmax_scaler.pkl'), 'rb'))\n",
    "        scaling_info = pickle.load(open(os.path.join(preprocess_params_dir, 'scaling_info.pkl'), 'rb'))\n",
    "        \n",
    "        # Extract scaling groups\n",
    "        log_features = scaling_info['log_features']\n",
    "        minmax_features = scaling_info['minmax_features']\n",
    "        robust_features = scaling_info['robust_features']\n",
    "        log_epsilon = scaling_info.get('log_epsilon', 1e-6)\n",
    "        \n",
    "        print(f\"Loaded preprocessing parameters from {preprocess_params_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading preprocessing parameters: {e}\")\n",
    "        # Default scaling groups if not loaded\n",
    "        log_features = ['Glucose', 'BUN', 'Creatinine', 'AST', 'ALT', 'ALP', 'Bilirubin', \n",
    "                        'Troponin', 'Lactate', 'PaO2', 'Urine', 'WBC', 'Platelets']\n",
    "        minmax_features = ['pH', 'FiO2', 'O2Sat', 'GCS', 'Age']\n",
    "        log_epsilon = 1e-6\n",
    "        robust_features = []\n",
    "    \n",
    "    # Identify columns to exclude from parameter list (metadata columns)\n",
    "    exclude_columns = ['PatientID', 'Hour', 'Time', 'Gender', 'ICUType', 'Age', 'Height', 'Weight']\n",
    "    \n",
    "    # Identify parameter columns (all columns that are not excluded)\n",
    "    param_columns = [col for col in grid_df.columns if col not in exclude_columns]\n",
    "    \n",
    "    if not robust_features:\n",
    "        robust_features = [col for col in param_columns \n",
    "                          if col not in log_features and col not in minmax_features]\n",
    "    \n",
    "    # Create parameter encoder\n",
    "    param_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "    param_encoder.fit(np.array(param_columns).reshape(-1, 1))\n",
    "    \n",
    "    # Determine max hour for time scaling (typically 48 from the original code)\n",
    "    max_hour = grid_df['Hour'].max()\n",
    "    \n",
    "    # Create triplets by melting the grid data\n",
    "    print(\"Converting grid data to triplets format...\")\n",
    "    \n",
    "    # Melt the DataFrame to long format (Hour, Parameter, Value)\n",
    "    melted_df = pd.melt(\n",
    "        grid_df, \n",
    "        id_vars=['PatientID', 'Hour'],\n",
    "        value_vars=param_columns,\n",
    "        var_name='Parameter',\n",
    "        value_name='v_original'\n",
    "    )\n",
    "    \n",
    "    # Remove rows with null values\n",
    "    melted_df = melted_df.dropna(subset=['v_original'])\n",
    "    \n",
    "    # Scale time to [0,1]\n",
    "    melted_df['t'] = melted_df['Hour'] / max_hour\n",
    "    \n",
    "    # Apply appropriate scaling to values based on parameter type\n",
    "    melted_df['v'] = melted_df['v_original']  # Default, will update for each scaling type\n",
    "    \n",
    "    # Apply log scaling for log features\n",
    "    for param in log_features:\n",
    "        if param in param_columns:\n",
    "            param_mask = melted_df['Parameter'] == param\n",
    "            # Only apply to positive values\n",
    "            valid_mask = param_mask & (melted_df['v_original'] > 0)\n",
    "            if valid_mask.sum() > 0:\n",
    "                melted_df.loc[valid_mask, 'v'] = np.log1p(\n",
    "                    melted_df.loc[valid_mask, 'v_original'].clip(lower=log_epsilon)\n",
    "                )\n",
    "    \n",
    "    # Apply MinMax scaling for minmax features\n",
    "    for param in minmax_features:\n",
    "        if param in param_columns:\n",
    "            param_mask = melted_df['Parameter'] == param\n",
    "            if param_mask.sum() > 0:\n",
    "                values = melted_df.loc[param_mask, 'v_original'].values.reshape(-1, 1)\n",
    "                if len(values) > 1:  # Need at least 2 values for proper scaling\n",
    "                    param_minmax = MinMaxScaler().fit(values)\n",
    "                    melted_df.loc[param_mask, 'v'] = param_minmax.transform(values).flatten()\n",
    "    \n",
    "    # Apply Robust scaling for robust features\n",
    "    for param in robust_features:\n",
    "        if param in param_columns:\n",
    "            param_mask = melted_df['Parameter'] == param\n",
    "            if param_mask.sum() > 0:\n",
    "                values = melted_df.loc[param_mask, 'v_original'].values.reshape(-1, 1)\n",
    "                if len(values) > 1:  # Need at least 2 values for proper scaling\n",
    "                    param_robust = RobustScaler().fit(values)\n",
    "                    melted_df.loc[param_mask, 'v'] = param_robust.transform(values).flatten()\n",
    "    \n",
    "    # Apply one-hot encoding for parameters\n",
    "    param_encoded = param_encoder.transform(melted_df['Parameter'].values.reshape(-1, 1))\n",
    "    \n",
    "    # Create a DataFrame with the one-hot encoded parameters\n",
    "    param_df = pd.DataFrame(\n",
    "        param_encoded, \n",
    "        columns=param_encoder.get_feature_names_out(),\n",
    "        index=melted_df.index\n",
    "    )\n",
    "    \n",
    "    # Concatenate with melted DataFrame\n",
    "    triplets_df = pd.concat([melted_df, param_df], axis=1)\n",
    "    \n",
    "    # Save the transformers for future use\n",
    "    transformers = {\n",
    "        'param_encoder': param_encoder,\n",
    "        'max_hour': max_hour,\n",
    "        'log_features': log_features,\n",
    "        'minmax_features': minmax_features,\n",
    "        'robust_features': robust_features,\n",
    "        'log_epsilon': log_epsilon\n",
    "    }\n",
    "    \n",
    "    # Save output if filename provided\n",
    "    if output_file:\n",
    "        triplets_df.to_csv(output_file, index=False)\n",
    "        print(f\"Triplets saved to {output_file}\")\n",
    "        \n",
    "        # Save transformers\n",
    "        pickle.dump(transformers, open(output_file.replace('.csv', '_transformers.pkl'), 'wb'))\n",
    "        print(f\"Transformers saved to {output_file.replace('.csv', '_transformers.pkl')}\")\n",
    "    \n",
    "    print(f\"Created {len(triplets_df)} triplets for {triplets_df['PatientID'].nunique()} patients\")\n",
    "    print(f\"DataFrame shape: {triplets_df.shape}\")\n",
    "    \n",
    "    return triplets_df, transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading grid data from /home/taekim/enhanced_preprocessed_data/enhanced_sampled_set-a.csv and converting to triplets\n",
      "Loaded preprocessing parameters from enhanced_preprocessed_data\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# Option 2: Convert from existing grid data\u001b[39;00m\n\u001b[32m      5\u001b[39m     grid_data_path = \u001b[33m'\u001b[39m\u001b[33m/home/taekim/enhanced_preprocessed_data/enhanced_sampled_set-a.csv\u001b[39m\u001b[33m'\u001b[39m  \u001b[38;5;66;03m# From previous processing\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     triplets_df, transformers = \u001b[43mload_and_convert_grid_to_triplets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m         \u001b[49m\u001b[43mgrid_data_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m         \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgrid_triplets_set-a.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m         \u001b[49m\u001b[43mpreprocess_params_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43menhanced_preprocessed_data\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Display triplet examples\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTriplet examples:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mload_and_convert_grid_to_triplets\u001b[39m\u001b[34m(grid_data_path, output_file, preprocess_params_dir)\u001b[39m\n\u001b[32m     49\u001b[39m     robust_features = [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m param_columns \n\u001b[32m     50\u001b[39m                       \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m log_features \u001b[38;5;129;01mand\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m minmax_features]\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Create parameter encoder\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m param_encoder = \u001b[43mOneHotEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mignore\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m param_encoder.fit(np.array(param_columns).reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Determine max hour for time scaling (typically 48 from the original code)\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Example usage (depending on whether starting from raw files or grid data)\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Option 2: Convert from existing grid data\n",
    "    grid_data_path = '/home/taekim/enhanced_preprocessed_data/enhanced_sampled_set-a.csv'  # From previous processing\n",
    "    triplets_df, transformers = load_and_convert_grid_to_triplets(\n",
    "         grid_data_path,\n",
    "         output_file='grid_triplets_set-a.csv',\n",
    "         preprocess_params_dir='enhanced_preprocessed_data'\n",
    "    )\n",
    "    \n",
    "    # Display triplet examples\n",
    "    print(\"\\nTriplet examples:\")\n",
    "    print(triplets_df.head())\n",
    "    \n",
    "    # Display information about the categorical encoding\n",
    "    if transformers:\n",
    "        print(\"\\nParameter encoding information:\")\n",
    "        print(f\"Number of parameters encoded: {len(transformers['param_encoder'].get_feature_names_out())}\")\n",
    "        print(f\"Encoded feature names: {transformers['param_encoder'].get_feature_names_out()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
