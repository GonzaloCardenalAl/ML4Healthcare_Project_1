{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, accuracy_score, f1_score\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths (adjust these based on your data location)\n",
    "train_data_path = \"/home/taekim/enhanced_preprocessed_data/enhanced_set-a.csv\"\n",
    "val_data_path = \"/home/taekim/enhanced_preprocessed_data/enhanced_set-b.csv\"\n",
    "test_data_path = \"/home/taekim/enhanced_preprocessed_data/enhanced_set-c.csv\"\n",
    "\n",
    "train_outcomes_path = \"/home/taekim/ml4h_data/p1/Outcomes-a.txt\"\n",
    "val_outcomes_path = \"/home/taekim/ml4h_data/p1/Outcomes-b.txt\"\n",
    "test_outcomes_path = \"/home/taekim/ml4h_data/p1/Outcomes-c.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        \n",
    "        # Register as buffer (not a parameter but part of the module)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to input\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2, dim_feedforward=128, \n",
    "                 dropout=0.1, output_dim=1, max_seq_length=1000):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        # Input projection to match transformer dimension\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_seq_length)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(d_model, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Project input features\n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # Global pooling and classification\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.squeeze(-1)\n",
    "        out = self.fc(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified calculate_class_weights function\n",
    "def calculate_class_weights(labels, print_info=False):\n",
    "    \"\"\"\n",
    "    Calculate class weights for imbalanced data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    labels : torch.Tensor or numpy.ndarray\n",
    "        The target labels (0 or 1)\n",
    "    print_info : bool, default=False\n",
    "        Whether to print class distribution info\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary mapping class indices to weights\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if it's not already\n",
    "    if not isinstance(labels, torch.Tensor):\n",
    "        labels = torch.tensor(labels)\n",
    "    \n",
    "    # Count instances of each class\n",
    "    class_counts = torch.bincount(labels.long())\n",
    "    \n",
    "    if print_info:\n",
    "        print(\"Class 0 (negative) count:\", class_counts[0].item())\n",
    "        print(\"Class 1 (positive) count:\", class_counts[1].item())\n",
    "    \n",
    "    # Calculate weights (inversely proportional to class frequency)\n",
    "    n_samples = len(labels)\n",
    "    n_classes = len(class_counts)\n",
    "    \n",
    "    # Formula: weight = n_samples / (n_classes * class_count)\n",
    "    weights = n_samples / (n_classes * class_counts.float())\n",
    "    \n",
    "    if print_info:\n",
    "        print(\"Class 0 weight:\", weights[0].item())\n",
    "        print(\"Class 1 weight:\", weights[1].item())\n",
    "    \n",
    "    # Return weights as a dictionary mapping class indices to weights\n",
    "    return {i: weights[i].item() for i in range(len(weights))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_transformer_model(model, train_loader, val_loader, learning_rate=0.0005, class_weights=None, num_epochs=50, patience=3):\n",
    "    \"\"\"\n",
    "    Train the Transformer model for binary classification.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The transformer model\n",
    "    train_loader : DataLoader\n",
    "        DataLoader for training data\n",
    "    val_loader : DataLoader\n",
    "        DataLoader for validation data\n",
    "    learning_rate : float, default=0.0005\n",
    "        Learning rate for optimizer\n",
    "    num_epochs : int, default=50\n",
    "        Maximum number of training epochs\n",
    "    patience : int, default=5\n",
    "        Number of epochs with no improvement after which training will be stopped\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : nn.Module\n",
    "        The trained transformer model\n",
    "    train_losses : list\n",
    "        List of training losses for each epoch\n",
    "    val_losses : list\n",
    "        List of validation losses for each epoch\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Create weight tensor for BCEWithLogitsLoss if class_weights provided\n",
    "    if class_weights is not None:\n",
    "        # For binary classification with BCEWithLogitsLoss, we need a single weight\n",
    "        # for the positive class (class 1)\n",
    "        pos_weight = torch.tensor([class_weights[1] / class_weights[0] ], device=device)\n",
    "        print(f\"Using positive class weight: {pos_weight.item():.4f}\")\n",
    "        \n",
    "        # Use weighted BCE loss for imbalanced classes\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    else:\n",
    "        # Use standard BCE loss if no weights provided\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-3)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, \n",
    "        step_size=10,     # Reduce LR every 10 epochs\n",
    "        gamma=0.5         # Multiply LR by 0.5 at each step\n",
    "    )\n",
    "    \n",
    "    # For tracking training progress\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Clip gradients to prevent exploding gradients (common in transformers)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_val_outputs = []\n",
    "        all_val_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                # Store outputs and targets for AUC calculation\n",
    "                all_val_outputs.append(torch.sigmoid(outputs).cpu())\n",
    "                all_val_targets.append(targets.cpu())\n",
    "        \n",
    "        # Calculate average validation loss\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Calculate validation AUC for monitoring\n",
    "        val_outputs = torch.cat(all_val_outputs).numpy().flatten()\n",
    "        val_targets = torch.cat(all_val_targets).numpy().flatten()\n",
    "        try:\n",
    "            from sklearn.metrics import roc_auc_score\n",
    "            val_auc = roc_auc_score(val_targets, val_outputs)\n",
    "            print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "        except:\n",
    "            val_auc = 0\n",
    "            print(\"Could not calculate validation AUC\")\n",
    "        \n",
    "        # Print training progress\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stopping_counter = 0\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), 'best_transformer_classifier.pth')\n",
    "            print(f\"Saved new best model with val_loss: {val_loss:.6f}\")\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= patience:\n",
    "                print(f\"Early stopping after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load('best_transformer_classifier.pth'))\n",
    "    \n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import necessary functions from your original code\n",
    "def load_data(time_series_path, outcomes_path):\n",
    "    \"\"\"Reuse the load_data function from your original code\"\"\"\n",
    "\n",
    "    time_series = pd.read_csv(time_series_path)\n",
    "    outcomes = pd.read_csv(outcomes_path)\n",
    "    outcomes = outcomes.rename(columns={'RecordID': 'PatientID'})\n",
    "    patient_outcomes = dict(zip(outcomes['PatientID'], outcomes['In-hospital_death']))\n",
    "    time_series['In-hospital_death'] = time_series['PatientID'].map(patient_outcomes)\n",
    "    time_series = time_series.dropna(subset=['In-hospital_death'])\n",
    "    return time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and merging data...\n"
     ]
    }
   ],
   "source": [
    "# 1. Load and merge the data with corresponding outcomes (reuse from LSTM code)\n",
    "print(\"Loading and merging data...\")\n",
    "# Reuse the load_data function from your original code\n",
    "train_data = load_data(train_data_path, train_outcomes_path)\n",
    "val_data = load_data(val_data_path, val_outcomes_path)\n",
    "test_data = load_data(test_data_path, test_outcomes_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def prepare_patient_sequences(data, seq_length=49):\n",
    "    \"\"\"Reuse the prepare_patient_sequences function from your original code\"\"\"\n",
    "\n",
    "    # Get all unique patient IDs\n",
    "    patient_ids = data['PatientID'].unique()\n",
    "    \n",
    "    # Define features (all columns except categorical and outcome columns)\n",
    "    exclude_cols = ['PatientID', 'Hours', 'Gender', 'ICUType', 'In-hospital_death']\n",
    "    feature_cols = [col for col in data.columns if col not in exclude_cols]\n",
    "    \n",
    "    sequences = []\n",
    "    targets = []\n",
    "    seq_patient_ids = []\n",
    "    \n",
    "    # Create one sequence per patient using the first seq_length hours\n",
    "    for patient_id in patient_ids:\n",
    "        # Get data for this patient\n",
    "        patient_data = data[data['PatientID'] == patient_id].sort_values('Hours')\n",
    "        \n",
    "        # Get the outcome for this patient\n",
    "        outcome = patient_data['In-hospital_death'].iloc[0]\n",
    "        \n",
    "        # Skip patients with insufficient data\n",
    "        if len(patient_data) < seq_length:\n",
    "            continue\n",
    "        \n",
    "        # Extract features for the first seq_length hours\n",
    "        X_patient = patient_data[feature_cols].values[:seq_length]\n",
    "        \n",
    "        # If less than seq_length hours, pad with zeros\n",
    "        if len(X_patient) < seq_length:\n",
    "            print(\"padding..\")\n",
    "            padding = np.zeros((seq_length - len(X_patient), len(feature_cols)))\n",
    "            X_patient = np.concatenate([X_patient, padding])\n",
    "        \n",
    "        sequences.append(X_patient)\n",
    "        targets.append(outcome)\n",
    "        seq_patient_ids.append(patient_id)\n",
    "    \n",
    "    return np.array(sequences), np.array(targets), np.array(seq_patient_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing sequences...\n"
     ]
    }
   ],
   "source": [
    "# 2. Prepare sequences (reuse the prepare_patient_sequences function)\n",
    "print(\"Preparing sequences...\")\n",
    "X_train, y_train, train_patient_ids = prepare_patient_sequences(train_data)\n",
    "X_val, y_val, val_patient_ids = prepare_patient_sequences(val_data)\n",
    "X_test, y_test, test_patient_ids = prepare_patient_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_simple(model, test_loader, threshold=0.5, save_figures=True, figure_prefix='model'):\n",
    "    \"\"\"\n",
    "    Simplified evaluation function that calculates AUROC, AUPRC, and generates\n",
    "    a confusion matrix heatmap.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The trained model\n",
    "    test_loader : DataLoader\n",
    "        DataLoader for test data\n",
    "    threshold : float, default=0.5\n",
    "        Threshold for binary classification\n",
    "    save_figures : bool, default=True\n",
    "        Whether to save the figures to disk\n",
    "    figure_prefix : str, default='model'\n",
    "        Prefix for saved figure filenames\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (metrics_dict, raw_predictions, binary_predictions, true_values)\n",
    "    \"\"\"\n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # For storing predictions and true values\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    # No gradient computation for evaluation\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            \n",
    "            # Store predictions and targets\n",
    "            all_preds.append(probs.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "    \n",
    "    # Concatenate batches\n",
    "    predictions = np.concatenate(all_preds).flatten()\n",
    "    true_values = np.concatenate(all_targets).flatten()\n",
    "    \n",
    "    # Convert to binary predictions\n",
    "    binary_predictions = (predictions >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate AUROC\n",
    "    try:\n",
    "        auroc = roc_auc_score(true_values, predictions)\n",
    "    except Exception as e:\n",
    "        print(f\"AUROC calculation failed: {str(e)}\")\n",
    "        auroc = np.nan\n",
    "    \n",
    "    # Calculate AUPRC\n",
    "    try:\n",
    "        auprc = average_precision_score(true_values, predictions)\n",
    "    except Exception as e:\n",
    "        print(f\"AUPRC calculation failed: {str(e)}\")\n",
    "        auprc = np.nan\n",
    "    \n",
    "    # Calculate accuracy and F1 score\n",
    "    accuracy = accuracy_score(true_values, binary_predictions)\n",
    "    f1 = f1_score(true_values, binary_predictions, zero_division=0)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(true_values, binary_predictions)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    if save_figures:\n",
    "        plt.savefig(f'{figure_prefix}_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Create ROC and PR curves\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    # ROC Curve\n",
    "    plt.subplot(1, 2, 1)\n",
    "    fpr, tpr, _ = roc_curve(true_values, predictions)\n",
    "    plt.plot(fpr, tpr, 'b-', label=f'AUROC = {auroc:.3f}')\n",
    "    plt.plot([0, 1], [0, 1], 'r--', label='Random')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(true_values, predictions)\n",
    "    plt.plot(recall_curve, precision_curve, 'g-', label=f'AUPRC = {auprc:.3f}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_figures:\n",
    "        plt.savefig(f'{figure_prefix}_roc_pr_curves.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()  # This will display figures in interactive environments like Jupyter\n",
    "    \n",
    "    # Create metrics dictionary\n",
    "    metrics = {\n",
    "        'AUC': auroc,\n",
    "        'AUPRC': auprc,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': cm\n",
    "    }\n",
    "    \n",
    "    return metrics, predictions, binary_predictions, true_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def run_transformer_experiment(train_data_path, val_data_path, test_data_path, \n",
    "                              train_outcomes_path, val_outcomes_path, test_outcomes_path,\n",
    "                              batch_size=1024, learning_rate=0.001, num_epochs=50):\n",
    "    \"\"\"\n",
    "    Run a complete transformer experiment from data loading to evaluation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data_path : str\n",
    "        Path to training time series data\n",
    "    val_data_path : str\n",
    "        Path to validation time series data\n",
    "    test_data_path : str\n",
    "        Path to test time series data\n",
    "    train_outcomes_path : str\n",
    "        Path to training outcomes data\n",
    "    val_outcomes_path : str\n",
    "        Path to validation outcomes data\n",
    "    test_outcomes_path : str\n",
    "        Path to test outcomes data\n",
    "    batch_size : int, default=32\n",
    "        Batch size for training (smaller than LSTM due to transformer's higher memory usage)\n",
    "    learning_rate : float, default=0.001\n",
    "        Learning rate for optimizer\n",
    "    num_epochs : int, default=50\n",
    "        Maximum number of training epochs\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : nn.Module\n",
    "        The trained transformer model\n",
    "    metrics : dict\n",
    "        Dictionary of evaluation metrics\n",
    "    train_losses : list\n",
    "        List of training losses for each epoch\n",
    "    val_losses : list\n",
    "        List of validation losses for each epoch\n",
    "    predictions : numpy.ndarray\n",
    "        Model predictions (probabilities)\n",
    "    true_values : numpy.ndarray\n",
    "        True target values\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # 3. Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1))\n",
    "\n",
    "     # 4. Calculate class weights using training data\n",
    "    class_weights = calculate_class_weights(y_train, print_info=True)\n",
    "    print(\"Class weights:\", class_weights)\n",
    "    \n",
    "    X_val_tensor = torch.FloatTensor(X_val)\n",
    "    y_val_tensor = torch.FloatTensor(y_val.reshape(-1, 1))\n",
    "    \n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_test_tensor = torch.FloatTensor(y_test.reshape(-1, 1))\n",
    "    \n",
    "    # 4. Create DataLoaders\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    for sample in train_loader:\n",
    "        print(sample)\n",
    "        break\n",
    "    \n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # 5. Create the transformer model\n",
    "    input_dim = X_train.shape[2]  # Number of features\n",
    "    output_dim = 1  # Binary classification\n",
    "    \n",
    "    print(\"Creating Transformer model...\")\n",
    "    model = TransformerClassifier(\n",
    "        input_dim=input_dim,\n",
    "        d_model=128,\n",
    "        nhead=4,\n",
    "        num_layers=3,\n",
    "        dim_feedforward=512,\n",
    "        dropout=0.3,\n",
    "        output_dim=output_dim\n",
    "    )\n",
    "    \n",
    "    # 6. Train the model\n",
    "    print(\"Training model...\")\n",
    "    model, train_losses, val_losses = train_transformer_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        learning_rate=learning_rate,\n",
    "        class_weights=class_weights,\n",
    "        num_epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    # 7. Evaluate on test set (reuse the evaluate_model function)\n",
    "    print(\"Evaluating model on test set...\")\n",
    "    metrics, predictions, binary_predictions, true_values = evaluate_model_simple(model, test_loader)\n",
    "    \n",
    "    # 8. Print metrics\n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    for metric, value in metrics.items():\n",
    "        if metric != 'Confusion Matrix':\n",
    "            print(f\"{metric}: {value:.6f}\")\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(metrics['Confusion Matrix'])\n",
    "    \n",
    "    # 9. Plot training and validation loss\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Transformer Model Training History')\n",
    "    plt.legend()\n",
    "    plt.savefig('transformer_training_history.png')\n",
    "    \n",
    "    # 10. Plot ROC curve if possible\n",
    "    try:\n",
    "        from sklearn.metrics import roc_curve\n",
    "        fpr, tpr, _ = roc_curve(true_values, predictions)\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(fpr, tpr, label=f'AUC = {metrics[\"AUC\"]:.3f}')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Transformer ROC Curve')\n",
    "        plt.legend()\n",
    "        plt.savefig('transformer_roc_curve.png')\n",
    "    except:\n",
    "        print(\"Could not generate ROC curve, possibly due to having only one class in the test set.\")\n",
    "    \n",
    "    return model, metrics, train_losses, val_losses, predictions, true_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cudagraphs', 'inductor', 'onnxrt', 'openxla', 'tvm']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch._dynamo.list_backends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Transformer Experiment ===\n",
      "Class 0 (negative) count: 3446\n",
      "Class 1 (positive) count: 554\n",
      "Class 0 weight: 0.5803830623626709\n",
      "Class 1 weight: 3.6101083755493164\n",
      "Class weights: {0: 0.5803830623626709, 1: 3.6101083755493164}\n",
      "[tensor([[[ 0.2267,  1.7918,  0.5306,  ...,  0.0000,  8.3303,  0.0000],\n",
      "         [ 0.2267,  1.7918,  0.5306,  ...,  0.0000,  8.3303,  0.0000],\n",
      "         [ 0.2267,  1.7918,  0.5306,  ...,  0.0000,  8.3303,  0.0000],\n",
      "         ...,\n",
      "         [ 0.2267,  1.7918,  0.5878,  ...,  0.0000, -2.6697,  0.0000],\n",
      "         [ 0.2267,  1.7918,  0.5878,  ...,  0.0000,  6.3303,  0.0000],\n",
      "         [ 0.2267,  1.7918,  0.5878,  ...,  0.0000,  4.3303,  0.0000]],\n",
      "\n",
      "        [[ 0.6667,  3.2581,  0.5878,  ..., -0.9950,  0.0000,  0.0000],\n",
      "         [ 0.6667,  3.2581,  0.5878,  ..., -0.9950,  0.0000,  0.0000],\n",
      "         [ 0.6667,  3.2581,  0.5878,  ..., -0.9950,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.6667,  3.0445,  0.4055,  ..., -1.0350,  0.0000,  0.0000],\n",
      "         [ 0.6667,  3.0445,  0.4055,  ..., -1.0350,  0.0000,  0.0000],\n",
      "         [ 0.6667,  3.0445,  0.4055,  ..., -1.0350,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.9067,  3.1355,  0.5878,  ..., -1.0250,  0.0000,  0.0000],\n",
      "         [ 0.9067,  3.1355,  0.5878,  ..., -1.0250,  0.0000,  0.0000],\n",
      "         [ 0.9067,  3.1355,  0.5878,  ..., -1.0250,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.9067,  2.7081,  0.4700,  ..., -1.0250,  0.0000,  0.0000],\n",
      "         [ 0.9067,  2.7081,  0.4700,  ..., -1.0250,  0.0000,  0.0000],\n",
      "         [ 0.9067,  2.7081,  0.4700,  ..., -1.0250,  0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.6533,  2.7081,  0.5306,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6533,  2.7081,  0.5306,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6533,  2.7081,  0.5306,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.6533,  2.6391,  0.6419,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6533,  2.6391,  0.6419,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6533,  2.6391,  0.6419,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.5867,  3.3673,  0.6931,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5867,  3.3673,  0.6931,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5867,  3.3673,  0.6931,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.5867,  3.4340,  0.6931,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5867,  3.4340,  0.6931,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5867,  3.4340,  0.6931,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.5467,  2.3979,  0.5306,  ...,  0.0000, -2.6697,  0.0000],\n",
      "         [ 0.5467,  2.3979,  0.5306,  ...,  0.0000, -2.6697,  0.0000],\n",
      "         [ 0.5467,  2.3979,  0.5306,  ...,  0.0000, -2.6697,  0.0000],\n",
      "         ...,\n",
      "         [ 0.5467,  2.0794,  0.5306,  ...,  0.0000, -2.6697,  0.0000],\n",
      "         [ 0.5467,  2.0794,  0.5306,  ...,  0.0000,  2.3303,  0.0000],\n",
      "         [ 0.5467,  2.0794,  0.5306,  ...,  0.0000, -3.6697,  0.0000]]]), tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        ...,\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])]\n",
      "Creating Transformer model...\n",
      "Training model...\n",
      "Using positive class weight: 6.2202\n",
      "Validation AUC: 0.5257\n",
      "Epoch 1/50\n",
      "Train Loss: 1.295587, Val Loss: 1.273579\n",
      "Saved new best model with val_loss: 1.273579\n",
      "Validation AUC: 0.5284\n",
      "Epoch 2/50\n",
      "Train Loss: 1.229645, Val Loss: 1.314792\n",
      "Validation AUC: 0.5305\n",
      "Epoch 3/50\n",
      "Train Loss: 1.212500, Val Loss: 1.333050\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    # Run transformer experiment\n",
    "    print(\"\\n=== Running Transformer Experiment ===\")\n",
    "    transformer_model, transformer_metrics, transformer_train_losses, transformer_val_losses, transformer_predictions, transformer_true_values = run_transformer_experiment(\n",
    "        train_data_path=train_data_path,\n",
    "        val_data_path=val_data_path,\n",
    "        test_data_path=test_data_path,\n",
    "        train_outcomes_path=train_outcomes_path,\n",
    "        val_outcomes_path=val_outcomes_path,\n",
    "        test_outcomes_path=test_outcomes_path,\n",
    "        batch_size=1024,  # Smaller batch size due to transformer's higher memory usage\n",
    "        learning_rate=1e-5,\n",
    "        num_epochs=50\n",
    "    )\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
