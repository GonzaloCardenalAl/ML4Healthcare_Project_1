{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, f1_score\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Processes input triplets (t, z, v) where:\n",
    "    - t: time (scaled to [0,1])\n",
    "    - z: categorical variable (one-hot encoded)\n",
    "    - v: observed value (scaled)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_variables=41, embedding_dim=64):\n",
    "        super(TripletEmbedding, self).__init__()\n",
    "        self.num_variables = num_variables\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Linear projection for time values\n",
    "        self.time_projection = nn.Linear(1, embedding_dim)\n",
    "        \n",
    "        # Embedding for variable categories\n",
    "        self.variable_embedding = nn.Embedding(num_variables, embedding_dim)\n",
    "        \n",
    "        # Linear projection for observed values\n",
    "        self.value_projection = nn.Linear(1, embedding_dim)\n",
    "        \n",
    "        # Final projection to combine all three embeddings\n",
    "        self.combined_projection = nn.Linear(3 * embedding_dim, embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape [batch_size, seq_length, 3]\n",
    "               where each triplet is (t, z, v)\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, _ = x.shape\n",
    "        \n",
    "        # Split the input into components\n",
    "        t = x[:, :, 0].unsqueeze(-1)  # [batch_size, seq_length, 1]\n",
    "        z = x[:, :, 1].long()          # [batch_size, seq_length]\n",
    "        v = x[:, :, 2].unsqueeze(-1)  # [batch_size, seq_length, 1]\n",
    "        \n",
    "        # Process each component\n",
    "        t_emb = self.time_projection(t)  # [batch_size, seq_length, embedding_dim]\n",
    "        z_emb = self.variable_embedding(z)  # [batch_size, seq_length, embedding_dim]\n",
    "        v_emb = self.value_projection(v)  # [batch_size, seq_length, embedding_dim]\n",
    "        \n",
    "        # Combine all embeddings\n",
    "        combined = torch.cat([t_emb, z_emb, v_emb], dim=2)  # [batch_size, seq_length, 3*embedding_dim]\n",
    "        \n",
    "        # Project to final embedding dimension\n",
    "        output = self.combined_projection(combined)  # [batch_size, seq_length, embedding_dim]\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        \n",
    "        # Register as buffer (not a parameter but part of the module)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to input\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletTransformerClassifier(nn.Module):\n",
    "    def __init__(self, num_variables=41, d_model=64, nhead=4, num_layers=2, \n",
    "                 dim_feedforward=128, dropout=0.1, output_dim=1, max_seq_length=5000):\n",
    "        super(TripletTransformerClassifier, self).__init__()\n",
    "        \n",
    "        # Triplet embedding layer\n",
    "        self.triplet_embedding = TripletEmbedding(num_variables, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_seq_length)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(d_model, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Process triplets into embeddings\n",
    "        x = self.triplet_embedding(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # Global pooling and classification\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.squeeze(-1)\n",
    "        out = self.fc(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_triplet_data(triplet_data_path, outcomes_path):\n",
    "    \"\"\"\n",
    "    Load triplet data from CSV file and outcomes.\n",
    "    \n",
    "    Args:\n",
    "        triplet_data_path: Path to CSV with triplet data\n",
    "        outcomes_path: Path to outcomes file\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with combined data\n",
    "    \"\"\"\n",
    "    # Load triplet data\n",
    "    triplet_data = pd.read_csv(triplet_data_path)\n",
    "    \n",
    "    # Load outcomes\n",
    "    outcomes = pd.read_csv(outcomes_path)\n",
    "    outcomes = outcomes.rename(columns={'RecordID': 'PatientID'})\n",
    "    \n",
    "    # Map outcomes to patients in triplet data\n",
    "    patient_outcomes = dict(zip(outcomes['PatientID'], outcomes['In-hospital_death']))\n",
    "    \n",
    "    # Return combined data\n",
    "    return triplet_data, patient_outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_patient_triplet_sequences(data, patient_outcomes, max_seq_length=5000):\n",
    "    \"\"\"\n",
    "    Prepare patient sequences from triplet data.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame with triplet data (PatientID, t, z, v)\n",
    "        patient_outcomes: Dictionary mapping PatientID to outcome\n",
    "        max_seq_length: Maximum sequence length to use\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (sequences, targets, patient_ids)\n",
    "    \"\"\"\n",
    "    # Get all unique patient IDs\n",
    "    patient_ids = data['PatientID'].unique()\n",
    "    \n",
    "    sequences = []\n",
    "    targets = []\n",
    "    seq_patient_ids = []\n",
    "    \n",
    "    # Create one sequence per patient using all triplets\n",
    "    for patient_id in patient_ids:\n",
    "        # Skip patients not in outcomes\n",
    "        if patient_id not in patient_outcomes:\n",
    "            continue\n",
    "            \n",
    "        # Get outcome for this patient\n",
    "        outcome = patient_outcomes[patient_id]\n",
    "        \n",
    "        # Get data for this patient\n",
    "        patient_data = data[data['PatientID'] == patient_id]\n",
    "        \n",
    "        # Extract triplets (t, z, v)\n",
    "        triplets = patient_data[['t', 'z', 'v']].values\n",
    "        \n",
    "        # Skip patients with no data\n",
    "        if len(triplets) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Limit sequence length if needed\n",
    "        if len(triplets) > max_seq_length:\n",
    "            triplets = triplets[:max_seq_length]\n",
    "        \n",
    "        # If less than max_seq_length triplets, pad with zeros\n",
    "        if len(triplets) < max_seq_length:\n",
    "            padding = np.zeros((max_seq_length - len(triplets), 3))\n",
    "            triplets = np.concatenate([triplets, padding])\n",
    "        \n",
    "        sequences.append(triplets)\n",
    "        targets.append(outcome)\n",
    "        seq_patient_ids.append(patient_id)\n",
    "    \n",
    "    return np.array(sequences), np.array(targets), np.array(seq_patient_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_transformer_model(model, train_loader, val_loader, learning_rate=0.0005, num_epochs=50, patience=5):\n",
    "    \"\"\"\n",
    "    Train the Transformer model for binary classification.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The transformer model\n",
    "    train_loader : DataLoader\n",
    "        DataLoader for training data\n",
    "    val_loader : DataLoader\n",
    "        DataLoader for validation data\n",
    "    learning_rate : float, default=0.0005\n",
    "        Learning rate for optimizer\n",
    "    num_epochs : int, default=50\n",
    "        Maximum number of training epochs\n",
    "    patience : int, default=5\n",
    "        Number of epochs with no improvement after which training will be stopped\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : nn.Module\n",
    "        The trained transformer model\n",
    "    train_losses : list\n",
    "        List of training losses for each epoch\n",
    "    val_losses : list\n",
    "        List of validation losses for each epoch\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Calculate positive weight based on class imbalance\n",
    "    pos_weight = torch.tensor([5.0])  # Adjust based on your class distribution\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-3)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, \n",
    "        step_size=10,     # Reduce LR every 10 epochs\n",
    "        gamma=0.5         # Multiply LR by 0.5 at each step\n",
    "    )\n",
    "    \n",
    "    # For tracking training progress\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Clip gradients to prevent exploding gradients (common in transformers)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_val_outputs = []\n",
    "        all_val_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                # Store outputs and targets for AUC calculation\n",
    "                all_val_outputs.append(torch.sigmoid(outputs).cpu())\n",
    "                all_val_targets.append(targets.cpu())\n",
    "        \n",
    "        # Calculate average validation loss\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Calculate validation AUC for monitoring\n",
    "        val_outputs = torch.cat(all_val_outputs).numpy().flatten()\n",
    "        val_targets = torch.cat(all_val_targets).numpy().flatten()\n",
    "        try:\n",
    "            val_auc = roc_auc_score(val_targets, val_outputs)\n",
    "            print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "        except:\n",
    "            val_auc = 0\n",
    "            print(\"Could not calculate validation AUC\")\n",
    "        \n",
    "        # Print training progress\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stopping_counter = 0\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), 'best_triplet_transformer.pth')\n",
    "            print(f\"Saved new best model with val_loss: {val_loss:.6f}\")\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= patience:\n",
    "                print(f\"Early stopping after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load('best_triplet_transformer.pth'))\n",
    "    \n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model_simple(model, test_loader, threshold=0.5, save_figures=True, figure_prefix='triplet_model'):\n",
    "    \"\"\"\n",
    "    Simplified evaluation function that calculates AUROC, AUPRC, and generates\n",
    "    a confusion matrix heatmap.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The trained model\n",
    "    test_loader : DataLoader\n",
    "        DataLoader for test data\n",
    "    threshold : float, default=0.5\n",
    "        Threshold for binary classification\n",
    "    save_figures : bool, default=True\n",
    "        Whether to save the figures to disk\n",
    "    figure_prefix : str, default='model'\n",
    "        Prefix for saved figure filenames\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (metrics_dict, raw_predictions, binary_predictions, true_values)\n",
    "    \"\"\"\n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # For storing predictions and true values\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    # No gradient computation for evaluation\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            \n",
    "            # Store predictions and targets\n",
    "            all_preds.append(probs.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "    \n",
    "    # Concatenate batches\n",
    "    predictions = np.concatenate(all_preds).flatten()\n",
    "    true_values = np.concatenate(all_targets).flatten()\n",
    "    \n",
    "    # Convert to binary predictions\n",
    "    binary_predictions = (predictions >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate AUROC\n",
    "    try:\n",
    "        auroc = roc_auc_score(true_values, predictions)\n",
    "    except Exception as e:\n",
    "        print(f\"AUROC calculation failed: {str(e)}\")\n",
    "        auroc = np.nan\n",
    "    \n",
    "    # Calculate AUPRC\n",
    "    try:\n",
    "        auprc = average_precision_score(true_values, predictions)\n",
    "    except Exception as e:\n",
    "        print(f\"AUPRC calculation failed: {str(e)}\")\n",
    "        auprc = np.nan\n",
    "    \n",
    "    # Calculate accuracy and F1 score\n",
    "    accuracy = accuracy_score(true_values, binary_predictions)\n",
    "    f1 = f1_score(true_values, binary_predictions, zero_division=0)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(true_values, binary_predictions)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    if save_figures:\n",
    "        plt.savefig(f'{figure_prefix}_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Create ROC and PR curves\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    # ROC Curve\n",
    "    plt.subplot(1, 2, 1)\n",
    "    fpr, tpr, _ = roc_curve(true_values, predictions)\n",
    "    plt.plot(fpr, tpr, 'b-', label=f'AUROC = {auroc:.3f}')\n",
    "    plt.plot([0, 1], [0, 1], 'r--', label='Random')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(true_values, predictions)\n",
    "    plt.plot(recall_curve, precision_curve, 'g-', label=f'AUPRC = {auprc:.3f}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_figures:\n",
    "        plt.savefig(f'{figure_prefix}_roc_pr_curves.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()  # This will display figures in interactive environments like Jupyter\n",
    "    \n",
    "    # Create metrics dictionary\n",
    "    metrics = {\n",
    "        'AUC': auroc,\n",
    "        'AUPRC': auprc,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': cm\n",
    "    }\n",
    "    \n",
    "    return metrics, predictions, binary_predictions, true_values\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_triplet_transformer_experiment(train_data_path, val_data_path, test_data_path, \n",
    "                                      train_outcomes_path, val_outcomes_path, test_outcomes_path,\n",
    "                                      batch_size=32, learning_rate=0.001, num_epochs=50, \n",
    "                                      max_seq_length=5000, num_variables=41):\n",
    "    \"\"\"\n",
    "    Run a complete transformer experiment from data loading to evaluation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data_path : str\n",
    "        Path to training triplet data\n",
    "    val_data_path : str\n",
    "        Path to validation triplet data\n",
    "    test_data_path : str\n",
    "        Path to test triplet data\n",
    "    train_outcomes_path : str\n",
    "        Path to training outcomes data\n",
    "    val_outcomes_path : str\n",
    "        Path to validation outcomes data\n",
    "    test_outcomes_path : str\n",
    "        Path to test outcomes data\n",
    "    batch_size : int, default=32\n",
    "        Batch size for training\n",
    "    learning_rate : float, default=0.001\n",
    "        Learning rate for optimizer\n",
    "    num_epochs : int, default=50\n",
    "        Maximum number of training epochs\n",
    "    max_seq_length : int, default=5000\n",
    "        Maximum sequence length to use\n",
    "    num_variables : int, default=41\n",
    "        Number of different variables in the dataset\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : nn.Module\n",
    "        The trained transformer model\n",
    "    metrics : dict\n",
    "        Dictionary of evaluation metrics\n",
    "    train_losses : list\n",
    "        List of training losses for each epoch\n",
    "    val_losses : list\n",
    "        List of validation losses for each epoch\n",
    "    predictions : numpy.ndarray\n",
    "        Model predictions (probabilities)\n",
    "    true_values : numpy.ndarray\n",
    "        True target values\n",
    "    \"\"\"\n",
    "    # 1. Load data\n",
    "    print(\"Loading data...\")\n",
    "    train_data, train_outcomes = load_triplet_data(train_data_path, train_outcomes_path)\n",
    "    val_data, val_outcomes = load_triplet_data(val_data_path, val_outcomes_path)\n",
    "    test_data, test_outcomes = load_triplet_data(test_data_path, test_outcomes_path)\n",
    "    \n",
    "    # 2. Prepare sequences\n",
    "    print(\"Preparing sequences...\")\n",
    "    X_train, y_train, train_patient_ids = prepare_patient_triplet_sequences(\n",
    "        train_data, train_outcomes, max_seq_length)\n",
    "    X_val, y_val, val_patient_ids = prepare_patient_triplet_sequences(\n",
    "        val_data, val_outcomes, max_seq_length)\n",
    "    X_test, y_test, test_patient_ids = prepare_patient_triplet_sequences(\n",
    "        test_data, test_outcomes, max_seq_length)\n",
    "\n",
    "    # 3. Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1))\n",
    "    \n",
    "    X_val_tensor = torch.FloatTensor(X_val)\n",
    "    y_val_tensor = torch.FloatTensor(y_val.reshape(-1, 1))\n",
    "    \n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_test_tensor = torch.FloatTensor(y_test.reshape(-1, 1))\n",
    "    \n",
    "    # 4. Create DataLoaders\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # 5. Create the transformer model\n",
    "    print(\"Creating Triplet Transformer model...\")\n",
    "    model = TripletTransformerClassifier(\n",
    "        num_variables=num_variables,\n",
    "        d_model=64,\n",
    "        nhead=4,\n",
    "        num_layers=2,\n",
    "        dim_feedforward=128,\n",
    "        dropout=0.3,\n",
    "        output_dim=1,\n",
    "        max_seq_length=max_seq_length\n",
    "    )\n",
    "    \n",
    "    # 6. Train the model\n",
    "    print(\"Training model...\")\n",
    "    model, train_losses, val_losses = train_transformer_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        learning_rate=learning_rate,\n",
    "        num_epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    # 7. Evaluate on test set\n",
    "    print(\"Evaluating model on test set...\")\n",
    "    metrics, predictions, binary_predictions, true_values = evaluate_model_simple(\n",
    "        model, test_loader, figure_prefix='triplet_transformer')\n",
    "    \n",
    "    # 8. Print metrics\n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    for metric, value in metrics.items():\n",
    "        if metric != 'Confusion Matrix':\n",
    "            print(f\"{metric}: {value:.6f}\")\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(metrics['Confusion Matrix'])\n",
    "    \n",
    "    # 9. Plot training and validation loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Triplet Transformer Model Training History')\n",
    "    plt.legend()\n",
    "    plt.savefig('triplet_transformer_training_history.png')\n",
    "    \n",
    "    return model, metrics, train_losses, val_losses, predictions, true_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m test_outcomes_path = \u001b[33m\"\u001b[39m\u001b[33m/home/taekim/ml4h_data/p1/Outcomes-c.txt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Run experiment\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m model, metrics, train_losses, val_losses, predictions, true_values = \u001b[43mrun_triplet_transformer_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_data_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_data_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_data_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_data_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_outcomes_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_outcomes_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_outcomes_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_outcomes_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_outcomes_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_outcomes_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_variables\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m41\u001b[39;49m\n\u001b[32m     25\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mrun_triplet_transformer_experiment\u001b[39m\u001b[34m(train_data_path, val_data_path, test_data_path, train_outcomes_path, val_outcomes_path, test_outcomes_path, batch_size, learning_rate, num_epochs, max_seq_length, num_variables)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# 1. Load data\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m train_data, train_outcomes = \u001b[43mload_triplet_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_outcomes_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m val_data, val_outcomes = load_triplet_data(val_data_path, val_outcomes_path)\n\u001b[32m     52\u001b[39m test_data, test_outcomes = load_triplet_data(test_data_path, test_outcomes_path)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mload_triplet_data\u001b[39m\u001b[34m(triplet_data_path, outcomes_path)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mLoad triplet data from CSV file and outcomes.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \u001b[33;03m    DataFrame with combined data\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Load triplet data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m triplet_data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtriplet_data_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Load outcomes\u001b[39;00m\n\u001b[32m     16\u001b[39m outcomes = pd.read_csv(outcomes_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:236\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    234\u001b[39m     chunks = \u001b[38;5;28mself\u001b[39m._reader.read_low_memory(nrows)\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     data = \u001b[43m_concatenate_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    239\u001b[39m     data = \u001b[38;5;28mself\u001b[39m._reader.read(nrows)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:376\u001b[39m, in \u001b[36m_concatenate_chunks\u001b[39m\u001b[34m(chunks)\u001b[39m\n\u001b[32m    374\u001b[39m     result[name] = union_categoricals(arrs, sort_categories=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     result[name] = \u001b[43mconcat_compat\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(non_cat_dtypes) > \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m result[name].dtype == np.dtype(\u001b[38;5;28mobject\u001b[39m):\n\u001b[32m    378\u001b[39m         warning_columns.append(\u001b[38;5;28mstr\u001b[39m(name))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/pandas/core/dtypes/concat.py:78\u001b[39m, in \u001b[36mconcat_compat\u001b[39m\u001b[34m(to_concat, axis, ea_compat_axis)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, np.ndarray):\n\u001b[32m     77\u001b[39m     to_concat_arrs = cast(\u001b[33m\"\u001b[39m\u001b[33mSequence[np.ndarray]\u001b[39m\u001b[33m\"\u001b[39m, to_concat)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_concat_arrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m to_concat_eas = cast(\u001b[33m\"\u001b[39m\u001b[33mSequence[ExtensionArray]\u001b[39m\u001b[33m\"\u001b[39m, to_concat)\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ea_compat_axis:\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# We have 1D objects, that don't support axis keyword\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define data paths\n",
    "    train_data_path = \"/home/taekim/enhanced_preprocessed_data/patient_triplets_set-a.csv\"\n",
    "    val_data_path = \"/home/taekim/enhanced_preprocessed_data/patient_triplets_set-b.csv\"\n",
    "    test_data_path = \"/home/taekim/enhanced_preprocessed_data/patient_triplets_set-c.csv\"\n",
    "    \n",
    "    train_outcomes_path = \"/home/taekim/ml4h_data/p1/Outcomes-a.txt\"\n",
    "    val_outcomes_path = \"/home/taekim/ml4h_data/p1/Outcomes-b.txt\"\n",
    "    test_outcomes_path = \"/home/taekim/ml4h_data/p1/Outcomes-c.txt\"\n",
    "    \n",
    "    # Run experiment\n",
    "    model, metrics, train_losses, val_losses, predictions, true_values = run_triplet_transformer_experiment(\n",
    "        train_data_path=train_data_path,\n",
    "        val_data_path=val_data_path,\n",
    "        test_data_path=test_data_path,\n",
    "        train_outcomes_path=train_outcomes_path,\n",
    "        val_outcomes_path=val_outcomes_path,\n",
    "        test_outcomes_path=test_outcomes_path,\n",
    "        batch_size=32,\n",
    "        learning_rate=0.001,\n",
    "        num_epochs=50,\n",
    "        max_seq_length=5000,\n",
    "        num_variables=41\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
